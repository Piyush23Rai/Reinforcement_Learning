# Multi-Agent RL: Mathematical Reference & Deep Dives
## Study Guide for Advanced Learners

---

## PART 1: FOUNDATIONAL MATHEMATICS

### Stochastic Games: Formal Definition

A stochastic game is a tuple **G = (N, S, A, P, R, Œ≥):**

- **N** = {1, 2, ..., n} = set of agents
- **S** = state space (finite or continuous)
- **A = A‚ÇÅ √ó A‚ÇÇ √ó ... √ó A‚Çô** = joint action space
- **P(s'|s, a‚ÇÅ, ..., a‚Çô)** = Markovian transition dynamics
- **R = {R‚ÇÅ, R‚ÇÇ, ..., R‚Çô}** where R·µ¢(s, a‚ÇÅ, ..., a‚Çô) = reward for agent i
- **Œ≥ ‚àà [0,1)** = discount factor

**Execution Sequence:**
```
At time t:
  State s_t ‚àà S
  All agents simultaneously choose: a_i,t ‚àà A_i
  Joint action: a_t = (a_1,t, ..., a_n,t)
  Rewards: r_i,t = R_i(s_t, a_1,t, ..., a_n,t)
  Transition: s_{t+1} ~ P(¬∑|s_t, a_t)
```

**Key Difference from MDP:**
- **MDP:** Only one agent acts. Environment stationary from observer's perspective.
- **Stochastic Game:** All agents act simultaneously. Non-stationary from every agent's view.

### Cumulative Returns and Nash Equilibrium

Agent i's cumulative return:
```
V_i(œÄ) = E[Œ£_{t=0}^‚àû Œ≥^t R_i(s_t, a_1,t, ..., a_n,t) | initial policy œÄ]
```

Notice: V_i depends on all policies, not just œÄ·µ¢.

**Definition (Pure Strategy Nash Equilibrium):**

A joint policy œÄ* = (œÄ‚ÇÅ*, œÄ‚ÇÇ*, ..., œÄ‚Çô*) is NE if for all agents i:
```
V_i(œÄ_i*, œÄ_{-i}*) ‚â• V_i(œÄ_i, œÄ_{-i}*)  for all alternative œÄ·µ¢
```

**Interpretation:** No agent improves by unilaterally deviating from their NE strategy.

---

## PART 2: MADDPG MATHEMATICS

### Single-Agent DDPG Baseline

Deterministic policy: a = Œº(s) (outputs single action, not distribution)

**Policy Gradient via Q-Function:**
```
‚àá_Œ∏ J = ùîº[‚àá_Œ∏ Œº(s) ¬∑ ‚àá_a Q(s, a)|_{a=Œº(s)}]
```

**Why Deterministic?** Direct chain rule allows gradient propagation through Q. More sample-efficient than stochastic.

### MADDPG Extension to Multi-Agent

Each agent i has:
- Deterministic actor: Œº·µ¢(o·µ¢) ‚Üí action ‚àà A·µ¢
- Centralized critic: Q·µ¢^cen(s, a‚ÇÅ, ..., a‚Çô) ‚Üí Q-value for agent i

**Policy Gradient for Agent i:**
```
‚àá_{Œ∏_i} J_i = ùîº[‚àá_{Œ∏_i} Œº_i(o_i) ¬∑ ‚àá_{a_i} Q_i^cen(s, a_1, ..., a_n)|_{a_j=Œº_j(o_j)}]
```

**Interpretation:** Actor Œº·µ¢ gets direction to move from critic's evaluation of joint action.

### MADDPG Algorithm (Pseudocode)

```
Initialize:
  For each agent i:
    Actor Œº·µ¢(o·µ¢; Œ∏Œº·µ¢), Target Œº·µ¢‚Åª
    Critic Q·µ¢(s, a; œÜ·µ¢), Target Q·µ¢‚Åª
  Replay buffer B

For episode = 1 to N:
  
  s‚ÇÄ ‚Üê environment.reset()
  
  For t = 0 to T:
    For each agent i:
      a·µ¢,‚Çú = Œº·µ¢(o·µ¢,‚Çú; Œ∏Œº·µ¢) + Œµ,  Œµ ~ ùí©(0, œÉ¬≤)
    
    Execute joint action ‚Üí (r‚ÇÅ,‚Çú, ..., r‚Çô,‚Çú), s‚Çú‚Çä‚ÇÅ
    Store (s_t, a_t, r_t, s_{t+1}) in B
  
  For K training iterations:
    
    For each agent i:
      Sample mini-batch M from B
      
      -- CRITIC UPDATE --
      For each (s, a, r, s') in M:
        y_i = r_i + Œ≥ Q_i‚Åª(s', Œº‚ÇÅ‚Åª(o‚ÇÅ'), ..., Œº‚Çô‚Åª(o‚Çô'))
        L_i^crit = (Q_i(s, a; œÜ·µ¢) - y_i)¬≤
      
      œÜ·µ¢ ‚Üê œÜ·µ¢ - Œ±_crit ‚àá_{œÜ·µ¢} L_i^crit
      
      -- ACTOR UPDATE --
      ‚àá_{Œ∏Œº·µ¢} J_i = ùîº[‚àá_{Œ∏Œº·µ¢} Œº·µ¢(o·µ¢) ¬∑ ‚àá_{a·µ¢} Q_i(s, a)]
      Œ∏Œº·µ¢ ‚Üê Œ∏Œº·µ¢ + Œ±_actor ‚àá_{Œ∏Œº·µ¢} J_i
      
      -- TARGET UPDATES --
      Œ∏Œº·µ¢‚Åª ‚Üê œÑ Œ∏Œº·µ¢ + (1-œÑ) Œ∏Œº·µ¢‚Åª
      œÜ·µ¢‚Åª ‚Üê œÑ œÜ·µ¢ + (1-œÑ) œÜ·µ¢‚Åª
```

**Hyperparameters:** Œ±_crit, Œ±_actor, œÑ (typically 0.001), œÉ (exploration noise), Œ≥ (0.99)

### Worked Example: 3-Store Pricing

**Setup:**
- Stores set prices p·µ¢ ‚àà [1, 10]
- Demand: D·µ¢ = 100 - 2p·µ¢ + 0.5(p‚±º + p‚Çñ)
- Profit: r·µ¢ = p·µ¢ ¬∑ D·µ¢

**Training Example:**

```
State: s = [D_1, D_2, D_3, inv_1, inv_2, inv_3]
       (recent demands + inventory)

Actions: a = [p_1, p_2, p_3] (prices)
         Œº_1(o_1) outputs p_1
         Œº_2(o_2) outputs p_2  
         Œº_3(o_3) outputs p_3

Execution:
  Store 1 sees o_1 = [D_1 history, inv_1]
  Outputs p_1 ‚âà 5.2 via Œº_1(o_1)
  
  Store 2 sees o_2 = [D_2 history, inv_2]
  Outputs p_2 ‚âà 4.8 via Œº_2(o_2)
  
  Store 3 outputs p_3 ‚âà 6.1

Demand computation:
  D_1 = 100 - 2(5.2) + 0.5(4.8 + 6.1) = 95.3
  D_2 = 100 - 2(4.8) + 0.5(5.2 + 6.1) = 99.2
  D_3 = 100 - 2(6.1) + 0.5(5.2 + 4.8) = 91.0

Profits:
  r_1 = 5.2 √ó 95.3 ‚âà 496
  r_2 = 4.8 √ó 99.2 ‚âà 477
  r_3 = 6.1 √ó 91.0 ‚âà 555

Critic Evaluation:
  Q_1(s, [5.2, 4.8, 6.1]) = estimated value for store 1's position
  Q_2(...) = estimated value for store 2
  Q_3(...) = estimated value for store 3

Target:
  y_1 = 496 + 0.99 √ó Q_1‚Åª(s', p_1', p_2', p_3')
  
Actor Update:
  ‚àá_{Œ∏Œº_1} Œº_1(o_1) gets direction from ‚àá_{a_1} Q_1(s, a)
  If critic says "increase p_1", actor increases Œº_1
  If critic says "decrease p_1", actor decreases Œº_1
```

---

## PART 3: MAPPO MATHEMATICS

### PPO Review (Single-Agent)

**Importance Sampling Ratio:**
```
r_t(Œ∏) = œÄ(a_t|s_t; Œ∏_new) / œÄ(a_t|s_t; Œ∏_old)
```

**Clipped Surrogate Loss:**
```
L^CLIP(Œ∏) = ùîº[min(r_t(Œ∏)√Ç_t, clip(r_t(Œ∏), 1-Œµ, 1+Œµ)√Ç_t)]
```

**Intuition:** If ratio r_t gets extreme, clipping prevents overshooting. Keeps policy update bounded.

### MAPPO Extension

Directly apply PPO to each agent:

**For Agent i:**
```
L^CLIP_i(Œ∏·µ¢) = ùîº[min(r_{i,t}(Œ∏·µ¢)√Ç_{i,t}, clip(r_{i,t}(Œ∏·µ¢), 1-Œµ, 1+Œµ)√Ç_{i,t})]

where:
  r_{i,t}(Œ∏·µ¢) = œÄ_i(a_{i,t}|o_{i,t}; Œ∏·µ¢) / œÄ_i(a_{i,t}|o_{i,t}; Œ∏·µ¢_old)
  √Ç_{i,t} = R_t - V^cen(s_t)
```

### MAPPO Algorithm (Pseudocode)

```
Initialize:
  For each agent i:
    Policy œÄ_i(a|o; Œ∏·µ¢)
  Value function V^cen(s; œÜ)
  Trajectory buffer T

For episode = 1 to N:
  
  s‚ÇÄ ‚Üê environment.reset()
  
  -- COLLECTION (On-policy) --
  For t = 0 to T:
    For each agent i:
      a_{i,t} ~ œÄ_i(¬∑|o_{i,t}; Œ∏·µ¢)
      log_prob_{i,t} = log œÄ_i(a_{i,t}|o_{i,t})
    
    Execute joint action ‚Üí rewards, s'
    Store (o, a, r, s, log_prob, V(s))

  -- ADVANTAGE COMPUTATION --
  For t = T down to 0:
    V_t = V^cen(s_t)
    Return: G_t = Œ£_{t'‚â•t} Œ≥^(t'-t) r_{t'}
    Advantage: √Ç_{i,t} = G_t - V_t
  
  -- TRAINING (K epochs) --
  For epoch = 1 to K:
    
    For mini-batch M in trajectory:
      
      For each agent i:
        
        -- Actor Update (PPO) --
        r_{i,t} = œÄ_i(a_{i,t}|o_{i,t}) / œÄ_i_old(a_{i,t}|o_{i,t})
        L_i = -min(r_{i,t} √Ç_{i,t}, clip(r_{i,t}, 1-Œµ, 1+Œµ) √Ç_{i,t})
        Œ∏·µ¢ ‚Üê Œ∏·µ¢ - Œ±_actor ‚àá_{Œ∏·µ¢} L_i
      
      -- Value Update (Supervised) --
      L_V = (V^cen(s) - G)¬≤
      œÜ ‚Üê œÜ - Œ±_value ‚àá_œÜ L_V
```

### Why Stochastic Policies in MAPPO?

MAPPO uses œÄ·µ¢(a·µ¢|o·µ¢), not deterministic Œº·µ¢(o·µ¢).

**Advantages:**
- Natural exploration: Policy entropy œÉ·µ¢ learned by network
- Avoids deterministic mode collapse (e.g., all stores setting same price)
- More stable in non-stationary settings
- Better convergence properties empirically

**Trade-off:** Lower sample efficiency (on-policy; don't reuse old data)

### Worked Example: Warehouse Inventory

```
State: s = [inventory at all warehouses, demand history]

Observations per warehouse i:
  o_i = [inventory_i, demand_history_i, seasonality]

Policy (Gaussian):
  œÄ_i(a_i|o_i) = ùí©(Œº_i(o_i), œÉ_i¬≤)
  
  Example:
    o_1 = [inventory=50, recent_demand=40, winter]
    Œº_1(o_1) outputs mean restock = 35
    œÉ_1 = 8 (std dev)
    Sample action: a_1 ~ ùí©(35, 8¬≤) = 42 (restock 42 units)

Reward:
  r_i = items_sold - restock_cost √ó a_i - penalty √ó max(0, demand - inventory)
  
  Example:
    With demand=45, inventory=50, restock=42:
    items_sold = min(45, 50+42) = 45
    cost = 5 √ó 42 = 210
    penalty = 0
    r_1 = 45 - 210 - 0 = -165

Central Value Function:
  V^cen(s) = expected cumulative reward from state s
  V^cen(s=current) = 500 (optimistic estimate)

Advantage:
  G_t = sum of rewards from t onward = -165 + ... = 200
  √Ç_1,t = G_t - V(s) = 200 - 500 = -300
  
Interpretation: Worse than expected. Warehouse 1's action (restock 42) was suboptimal.

PPO Update:
  Old policy outputted a_1 ~ ùí©(35, 8¬≤), got a_1=42
  New policy outputs Œº'_1(o_1) = 32 (lower mean)
  
  r_t = œÄ_new(42|...) / œÄ_old(42|...) ‚âà 0.8 (slightly less likely under new)
  
  L = -min(0.8 √ó (-300), clip(0.8, 0.8, 1.2) √ó (-300))
    = -min(-240, -240) = 240 (loss; will decrease policy weight)
  
  Policy update: reduce probability of high restock values.
```

---

## PART 4: QMIX MATHEMATICS

### Value Function Factorization

**Goal:** Decompose joint value function using individual values:
```
Q_total(s, a_1, ..., a_n) = f(Q_1(o_1, a_1), Q_2(o_2, a_2), ..., Q_n(o_n, a_n) | s)
```

**Benefit:** If each agent independently maximizes its Q·µ¢, joint action maximizes Q_total.

### Monotonicity Constraint (Critical)

```
‚àÇQ_total/‚àÇQ_i ‚â• 0  for all agents i
```

**Theorem:** If Q_total is monotonic in all Q·µ¢, then:
```
argmax_{a_1,...,a_n} Q_total(s, a) = (argmax_{a_1} Q_1(o_1, a_1),
                                      argmax_{a_2} Q_2(o_2, a_2),
                                      ...
                                      argmax_{a_n} Q_n(o_n, a_n))
```

**Proof Sketch:** If Q·µ¢ ‚â§ Q·µ¢* and Q_total is increasing in Q·µ¢, then Q_total ‚â§ Q_total* via monotonicity. Thus greedy choices on Q·µ¢ lead to greedy on Q_total.

### QMIX Mixing Network Architecture

```
Individual Q-Networks:              Mixing Network:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ           ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
o_i ‚Üí Neural Network ‚Üí Q_i(a_i)     Input: [Q_1, Q_2, ..., Q_n], state s
                                     
                                     Layer 1:
                                       w1_raw = Neural Network(s)
                                       w1 = abs(w1_raw)  ‚Üê ensures w ‚â• 0
                                       b1 = Neural Network(s)
                                       hidden = ReLU(w1 ‚äô [Q_1, Q_2, ...] + b1)
                                     
                                     Layer 2:
                                       w2 = abs(Neural Network(s))
                                       b2 = Neural Network(s)
                                       Q_total = w2 ‚äô hidden + b2

Target:  y = r + Œ≥ Q_total'
```

**Key Trick:** `abs()` on weights enforces w ‚â• 0. Combined with ReLU, ensures ‚àÇQ_total/‚àÇQ·µ¢ ‚â• 0.

### QMIX Algorithm

```
Initialize:
  For each agent i: Q_i(o_i, a_i; œà_i), Target Q_i‚Åª
  Mixing network M(Q_1,...,Q_n,s; Œæ), Target M‚Åª
  Replay buffer B

For episode = 1 to N:
  
  s‚ÇÄ ‚Üê environment.reset()
  
  For t = 0 to T:
    For each agent i:
      a_{i,t} = Œµ-greedy on Q_i(o_{i,t}, ¬∑; œà_i)
    
    Execute joint action ‚Üí r_t, s_{t+1}
    Store (s_t, a_t, r_t, s_{t+1}) in B

  For K training iterations:
    
    Sample mini-batch M from B
    
    -- COMPUTE Q_TOTAL --
    For each (s, a, r, s') in M:
      
      Forward:
        Q_i = Q_i(o_i, a_i; œà_i) for each i
        Q_total = M([Q_1, ..., Q_n], s; Œæ)
      
      Target:
        Q_i' = Q_i‚Åª(o_i', argmax_{a'_i} Q_i(o_i', a'_i); œà_i‚Åª)
        Q_total' = M‚Åª([Q_1', ..., Q_n'], s'; Œæ‚Åª)
        y = r + Œ≥ Q_total'
    
    -- LOSS (All agents + mixing jointly) --
    L = (Q_total - y)¬≤
    
    -- UPDATE --
    For each i: œà_i ‚Üê œà_i - Œ± ‚àá_{œà·µ¢} L
    Œæ ‚Üê Œæ - Œ± ‚àá_Œæ L
    
    -- TARGET UPDATES --
    œà_i‚Åª ‚Üê œÑ œà_i + (1-œÑ) œà_i‚Åª
    Œæ‚Åª ‚Üê œÑ Œæ + (1-œÑ) Œæ‚Åª
```

### Worked Example: Store Fulfillment

```
Setup: 10 stores, order arrives at location X.
       One store must fulfill (constraint: Œ£ a·µ¢ = 1).

Observations per store i:
  o_i = [inventory_i, distance_to_X, current_load_i]

Individual Q-Values:
  Q_i(o_i, a·µ¢=1) = -cost(i, X) - penalty(inventory_i)
  Q_i(o_i, a·µ¢=0) = 0
  
  Example costs:
    Store A (close): cost = 2 km, Q_A(1) = -2
    Store B (medium): cost = 5 km, Q_B(1) = -5
    Store C (far): cost = 15 km, Q_C(1) = -15
    Store D (close, low inv): cost = 3 km + 10 penalty, Q_D(1) = -13

Mixing Network learns:
  Q_total = -min(|Q_1|, |Q_2|, |Q_3|, |Q_4|)
  (approximately; picks best store)

Decentralized Execution:
  Store A: Q_A(1) = -2 (best)
  Store B: Q_B(1) = -5
  Store C: Q_C(1) = -15
  Store D: Q_D(1) = -13
  
  Greedy: Store A maximizes (least negative = best)
  
Monotonicity Guarantee:
  If mixing network is monotonic in Q_i:
  argmax_i Q_i(o_i, 1) = argmax Q_total
  
  Store A's greedy choice = globally optimal!
  No coordination needed; decentralized exec works.
```

---

## PART 5: CONVERGENCE & THEORY

### MADDPG Convergence (Informal)

**Theorem (Lowe et al. 2017):**
Under assumptions:
1. Sufficient exploration (all states visited)
2. Critic function approximation bounded
3. Other agents' policies quasi-static (slow change)
4. Œ≥ < 1

‚Üí MADDPG converges to local Nash Equilibrium of stochastic game.

**Critical Issues:**
- Assumption 3 violated (agents change simultaneously)
- Local NE may be exponentially many
- No guarantee on which NE emerges
- Empirically can oscillate or diverge

### MAPPO Convergence (Empirical)

- Empirically converges to stationary policy in cooperative settings
- Stronger convergence than independent learners (non-stationary)
- Weaker theory than MADDPG
- Convergence depends on problem structure

**Recommendation:** Use MAPPO for empirical stability, not for convergence theory.

### QMIX Convergence (Formal)

**Theorem (Rashid et al. 2020):**
If agents use independent Q-learning on their Q·µ¢ and mixing network learns monotonic decomposition with sufficient exploration:
‚Üí QMIX converges to optimal cooperative policy.

**Key Advantage:** QMIX provides formal guarantees for cooperative settings, unlike MADDPG/MAPPO.

---

## PART 6: EQUATION SUMMARY

### Quick Reference

**Stochastic Game Bellman (Implicit, for Nash):**
```
For NE: V_i*(s) = ùîº_{a~œÄ*}[R_i(s,a) + Œ≥ V_i*(s')]
```

**MADDPG:**
```
Critic:  Q_i^cen(s, a) ‚Üí predict value for agent i
Actor:   œÄ_i(a|o) = Œ¥(a - Œº_i(o))  (deterministic)
Update:  ‚àá_Œ∏·µ¢ J_i = ùîº[‚àá_{a_i} Q_i ‚àá_{Œ∏·µ¢} Œº_i]
Target:  y_i = r_i + Œ≥ Q_i‚Åª(s', a')
```

**MAPPO:**
```
Policy:    œÄ_i(a|o) = stochastic (e.g., Gaussian)
Value:     V^cen(s) = baseline for advantage
Advantage: √Ç_i = G - V^cen(s)  (return minus baseline)
Loss:      L = -min(r¬∑√Ç, clip(r, 1¬±Œµ)¬∑√Ç)
Update:    PPO clipped policy gradient
```

**QMIX:**
```
Individual Q: Q_i(o_i, a_i) ‚Üí value for agent i's action
Mixing:       Q_total = M([Q_1, ..., Q_n], s)  (monotonic)
Target:       y = r + Œ≥ Q_total'(s', a'_i)
Constraint:   ‚àÇQ_total/‚àÇQ_i ‚â• 0  (monotonicity)
Execution:    a_i* = argmax Q_i(o_i, a_i)  independently
```

---

## PART 7: DEBUGGING & COMMON ISSUES

### Issue 1: Training Loss Increases

**Symptom:** Loss increases instead of decreasing.
**Cause:** Non-stationary environment. Critic trained to evaluate old policies; now facing new policies.
**Fix:** 
- MADDPG: Increase replay buffer size, reduce learning rates, smaller œÑ
- MAPPO: Check advantage computation; ensure returns are computed correctly

### Issue 2: Agents Don't Improve

**Symptom:** Rewards plateau early.
**Cause:** Sparse rewards (monthly feedback) or poor credit assignment.
**Fix:**
- Design intermediate rewards (daily/weekly signals)
- Use reward shaping
- In MAPPO, check entropy; ensure exploration not collapsed

### Issue 3: Policy Oscillates

**Symptom:** Agents alternate between actions; never settle.
**Cause:** Competitive dynamics or deterministic mode collapse.
**Fix:**
- MADDPG: Use MAPPO instead (stochasticity helps)
- MAPPO: Check PPO clipping; ensure Œµ large enough
- Add constraints to action space (e.g., min price floors)

### Issue 4: Critic Overfits

**Symptom:** Good training loss, poor execution.
**Cause:** Critic saw full state; actor only sees partial observation.
**Fix:**
- Design observations to be sufficient (include relevant features)
- During training, sometimes mask information to match execution-time visibility
- Monitor train vs. test performance separately

---

## PART 8: RESEARCH CONNECTIONS

**Non-Stationarity & Convergence:**
- Key open problem in MARL
- Connection to game theory (perfect vs. imperfect info games)
- Potential solution: explicit modeling of opponent learning

**Credit Assignment:**
- Connection to interpretability (which agent helped?)
- Potential solution: counterfactual explanations ("what if i acted differently?")

**Scaling:**
- Mean-field games approximate many agents with average behavior
- Graph neural networks encode local interactions

**Safety:**
- Constrained RL: formally enforce constraints
- Constitutional AI: encode values as inviolable rules

---

**End of Mathematical Reference**

Use alongside HTML module and Instructor Guide for comprehensive technical understanding.
